# Open Observability Results Standard (OORS)

Champion: Jean-Georges Perrin (jgp).

Slack: https://data-mesh-learning.slack.com/archives/C08NDBWHRD4.

Jira: https://bitol-io.atlassian.net/jira/software/projects/OORS/boards/68.

## Summary

Provide an easy mechanism to exchange observability results.

## Motivation

A simpler format like OORS (Open Observability Results Standard) is useful because it reduces the complexity of capturing and sharing essential observability data across systems. By focusing on clarity, portability, and ease of implementation, OORS enables teams to communicate operational results—such as SLAs, uptime, latency, and data freshness—without requiring deep observability expertise or complex infrastructure. It supports consistent reporting and automation across the data lifecycle, making it easier to track the health and performance of data products. As an open format developed within the Bitol community, OORS is lightweight, human-readable, following the same philosophy as OCDS & ODPS, and designed to integrate naturally into existing data workflows, while remaining independent and adaptable to various environments.

OpenTelemetry is a powerful and flexible observability framework, but it comes with challenges such as a steep learning curve, complex configuration, and evolving support for logs. It lacks a plug-and-play experience, requiring manual setup and integration, especially for legacy systems or vendor exporters. Operational overhead, data volume concerns, and potential resource consumption can add to complexity, while security and governance features must be implemented externally. Despite these drawbacks, its vendor-neutral approach and growing ecosystem make it a strong choice for organizations willing to invest in proper implementation.

## Design and examples

### Design goals

- Lightweight: Flat, concise structure; easy to print, log, send over HTTP, or push to a queue, etc.
- Tool‑agnostic: Can wrap results coming from OpenTelemetry, vendor tools, ETL-pipelines, (Custom build) Applications or (ad‑hoc) scripts, etc.
- Contract‑aware: Optional but first‑class linkage to ODCS/ODPS IDs so data products can expose their health in a consistent way.
- Result‑centric: Only carries outcomes of checks (pass/fail, measured values, breach), not raw metrics or traces.
​
### Core primitives

_Fundamental primitives_

`apiVersion`: Initial version `v0.1.0` (to get started)

`kind`: Clear goal for OORS is `ObservabilityResults`

`id`: A unique identifier (could be UUID) identifying the specific Observability Results.

`observedAt`: The moment in time when the observations where done (in ISO 8601 format)

_core objects_

`source`: An object describing the source from where the results come from. This is context about for example the _job_, _pipeline_, etc. that made the observations.
- `process`: The name of the process (job, pipeline, etc.) that made the observations.
- `runID`: An optional unique identifier for the specific run/execution of the process (e.g., a build ID, run ID, etc.).
- `vendor`: An optional field indicating the vendor or tool that produced the observations (e.g., `soda`, `dbt`, `inhouse-build`, etc.).

`results`: An array of results (at least 1).

`result`: A single result object. This object carries the observation details.

_Optional objects_

`contract`: The `contract` object is optional (but highly recommended) to allow for observations that are not directly tied to a specific contract. This flexibility enables reporting on ad-hoc checks or observations that may not have been predefined in a contract, while still encouraging linkage to contracts when applicable for better traceability and context. The object contains the following fields:
- `contractID`: A unique identifier (could be UUID) identifying the specific contract (ODCS/DCS/ODPS) that provided the specifications for the observations.
- `standard`: The standard followed by the contract, e.g., `ODCS`, `ODPS`, etc.
- `version`: The version of the contract (not the standard), e.g., `1.0.0`, `2.0.0`, etc.

__NOTE__: When a contract is provided, it is expected that all the observations in the `results` array are based on the specifications defined in that contract. No other observations should be included in the same message when a contract is specified. Additional observations not related to the contract should be sent in separate OORS messages (without the `contract` object).

_Result object details_

Each `result` object contains the following fields:
- `id`: A unique identifier for the specific check/metric.
- `type`: The type of observation, either `check` (for pass/fail checks) or `metric` (for measured values).
- `name`: A human-readable name for the check/metric.
- `status`: The outcome of the observation, either `pass` or `fail`.
- `severity`: The severity level of the observation, e.g., `critical`, `high`, `medium`, `low`, `warning`, etc.
- `dimension`: The dimension of data quality being observed, e.g., `accuracy`, `completeness`, `consistency`, `timeliness`, `validity`, `uniqueness`, etc. (following ODCS data quality dimensions).
- `target`: An object describing the resource being observed, including:
  - `resourceType`: The type of resource, e.g., `table`, `column`, `dataset`, etc.
  - `resourceIdentifier`: The identifier of the resource, e.g., table name, column name, etc.
- `measure`: An object describing the measurement details, including:
  - `metric`: The name of the metric being measured, e.g., `rowCount`, `age`, `latency`, etc.
  - `value`: The observed value of the metric.
  - `threshold`: An object describing the threshold criteria, e.g., `mustBe`, `mustBeLowerOrEqual`, etc. (following ODCS Operator definitions).
  - `unit`: The unit of measurement, e.g., `rows`, `seconds`, `percent`, etc.
- `message`: A human-readable message providing (additional) context about the observation.


#### Example observation message

Single observation message with a single result (no contract reference):
```json
{
    "apiVersion": "v0.1.0",
    "kind": "ObservabilityResults",
    "observedAt": "2025-12-25T17:00:00+01:00",
    "source": {
        "process": "order-preprocessor-dbt",
        "runID": "27a3de3d",
        "vendor": "inhouse-build"
    },
    "results": [
        {
            "id": "business_sense_myTable",
            "type": "check",
            "name": "All column values in a single row should make business sense",
            "status": "fail",
            "severity": "warning",
            "dimension": "validity",
            "target": {
                "resourceType": "column",
                "resourceIdentifier": ["myTable.Column1", "myTable.Column2", "myTable.Column4"]
            },
            "measure": {
                "metric": "rowCount",
                "value": 12.66,
                "threshold": {
                    "mustBeLowerOrEqual": 5
                },
                "unit": "percent"
            },
            "message": "To many rows in MyTable do not make business sense!"
        }
    ]
}
```

Single observation message with multiple results (and contract reference):
```json
{
    "apiVersion": "v0.1.0",
    "kind": "ObservabilityResults",
    "observedAt": "2025-12-25T18:00:00+01:00",
    "source": {
        "process": "order-processor-dbt",
        "runID": "268e5b0f",
        "vendor": "inhouse-build"
    },
    "results": [
        {
            "id": "dup_rows_myTable",
            "type": "check",
            "name": "No duplicate rows",
            "status": "fail",
            "severity": "critical",
            "dimension": "uniqueness",
            "target": {
                "resourceType": "table",
                "resourceIdentifier": "myTable"
            },
            "measure": {
                "metric": "rowCount",
                "value": 3,
                "threshold": {
                    "mustBe": 0
                },
                "unit": "rows"
            },
            "message": "Multiple duplicate rows found in MyTable!"
        },
        {
            "id": "freshness_myDataProduct",
            "type": "metric",
            "name": "Data should not be stale",
            "status": "fail",
            "severity": "high",
            "dimension": "timeliness",
            "target": {
                "resourceType": "column",
                "resourceIdentifier": "myTable.UpdatedTS"
            },
            "measure": {
                "metric": "age",
                "value": 4192,
                "threshold": {
                    "mustBeLowerOrEqual": 3600
                },
                "unit": "seconds"
            },
            "message": "Data has not been updated in the last hour!"
        }
    ],
    "contract": {
        "contractID": "949f39d3-59e9-4a2b-a53a-d8439ac9506a",
        "standard": "ODCS",
        "version": "1.0.0"
    }
}
```

### Questions

__Should we allow for multiple results (a results array), or should we limit to each observation being it's own message?__

Reason for an array is that a process, being a pipeline, a program, an external system, can collect multiple observations of a _processing run_ into a single message. (Obvious/typical) Downside is that if for whatever reason a process does not come to the point of sending the collected observations (due to an error/crash), earlier observations do not get send at all. This would be _the_ argument to sent each observation individually directly after the observation is done.

_Prefence_: using a `results` array with a minimum lengt of 1, a single observation. This allows for sending single observations for those processes that prefer this.

__Do we only allow observations on checks that have been defined in a contract?__

It could be argued that all checks should be specified at the contract level, so it is totally clear from the contract what could be expected and which observations could be done. When following this argument, the _contract_ object is not optional but mandatory! With a clear contract specifying the contraints and checks, it is possible to anticipate certain observation and build logic on how to handle them.

On the other hand, being _open to unspecified observations_ allows for easier signaling of any observation from any process for whatever reason (where relevance is always true for the process doing the observation). This would make it much easier to start with reporting observations. Any observation bears some relevance to the observer but also to the provider/system causing the observation.

_Preference_: Be open to _unspecified observations_ lowering the bearrier to report observations. As any observation usually helps to improve (even by starting the discussion around it).

__Should we retrieve the meta details of a check afterward from the contract or _duplicate_ them in the observation results?__

If/when observations are found based on the specifications from a contract, it is clear what the meta data is as it is within the contract. To make the observation message smaller, the metadata could be omitted as it can be retrieved afterwards by getting it from the contract. This is ofcourse only possible if an observation is related to the specifications of the contract.

For observations not related to a contract, meta data should always be added to the observation.

So either the observation contains a reference to the contract for later retrieval of the metadata or it contains the metadata when the observation is not related to a contract. So an observation result always has one or the other.

By making the metadata mandatory for all observations, it becomes easier to process the observations and no needs to add metadata afterwards. This makes each observation more verbose and self-explaining as it always contain all relevant metadata.

_Preference_: Make the metadata mandatory for all observations and duplicate metadata from a contract when the observation is done based on the contract specification. This makes each observation fully self-contained and self-explanatory.

__The Observations should only contain observation about _non-compliance_ and not positive results?__

Obviously, observations should be about those observations that indicate non-compliance with the set expectations. But, it could be argued that reporting on compliant observations provides additional insights as it can be reported if/when and which checks have been performed. Also providing an indication when the last time a certain check has been performed. Compliant observations can also be used to indicate when previously non-compliant objects are now compliant again.

Biggest _downside_ of this is the possible shear amount of observations as it could be expected that many downstream processes execute the same/similar checks on the same objects and most check are expected to signal compliancy.

There are many alternative ways to gain insights in which checks have been executed when.

_Preference_: Only _non-compliant_ observations as those are the ones that are of real interest and need proper follow-up action.

### Transport

An OORS-signal can be send in either JSON or YAML format over HTTP(S), streams/queue's, stdout, or any other transport mechanisme suitable.

## Alternatives

- Using Open Telemetry directly
Open Telemetry is expressive and already standardizes telemetry, but it is intentionally low‑level and designed for traces/metrics/logs, not “business‑oriented observability results” like “SLA breach for ODCS contract X”.

    Mapping high‑level data‑product checks into OTLP often requires custom conventions and still leaves consumers to re‑assemble context from multiple signals.

- Tool‑specific result formats
Many vendors (e.g., Soda, dbt, platform‑specific DQ engines) have their own JSON/YAML formats and APIs for test results.

    These work well in isolation but make cross‑platform aggregation, governance, and catalog integration harder, especially in heterogeneous data meshes.

- Enrich ODCS/ODPS with inline results
ODCS/ODPS already defines SLAs and data quality rules; embedding results into contracts would blur “specification” and “runtime state”.

    Keeping OORS separate but linkable preserves ODCS/ODPS as declarative specs while OORS captures time‑varying observations.

## Decision

> The decision made by the TSC.

## Consequences

> The consequences of this decision.

## References

> Prior art, inspiration, and other references you used to create this based on what's worked well before.

* [OpenTelemetry](https://opentelemetry.io/).
